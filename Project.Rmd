---
title: "Project"
author: "Gabriela Maślanka, Karolina Gajewska"
date: "`r Sys.Date()`"
output:
  html_document: 
    code_folding: hide
    theme: paper
---
### Wstęp

Celem niniejszej pracy jest osiągnięcie jak najbardziej precyzyjnej prognozy dotyczącej charakteru nowotworu piersi u kobiet, opartej na zebranych danych dotyczących zmian nowotworowych.

Biblioteki użyte w projekcie:
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(psych)
library(corrplot)
library(e1071)
library(caret)
library(randomForest)
library(ROSE)
library(smotefamily)
library(DALEX)
```

### Opis danych 
```{r}
data<-read.csv("Breast_cancer_data.csv", header = TRUE, sep = ",",dec = ".")
head(data)
```

Dane na których możemy budować nasz model to:

**zmienne objaśniające**:

**mean_radius**: Średni rozmiar guza, istotny dla oceny jego wielkości.

**mean_texture**: Jednorodność komórek guza, mierzona wartością tekstury.

**mean_perimeter**: Zewnętrzna długość krzywej guza, informująca o jego kształcie.

**mean_area**: Przestrzenny rozmiar guza, wyrażony jako średnia powierzchnia.

**mean_smoothness**: Stopień równomierności komórek, określający gładkość guza.

**Zmienna objaśniana:**

**diagnosis**: Zmienna binarna (0-1) określająca czy zmiana nowotworowa zostałą rozpoznana jako łagodna (0) czy złośliwa (1) .

##### Analiza wpływu poszczególnych zmiennych na zmienną prognozowaną
Na początku sprawdzamy, czy w naszym zbiorze danych zmienne objaśniające mają wpływ na zmienną objaśnianą. Wykorzystujemy do tego model regresji logistycznej.
```{r message=FALSE, warning=FALSE}
set.seed(1)

data$diagnosis<- as.factor(data$diagnosis)
index <- sample(nrow(data), 450, replace = F)
train <- data[index,]
test <- data[-index,]

model <- glm(diagnosis ~ mean_radius + mean_texture + mean_perimeter + mean_area + mean_smoothness, 
             data, 
             family = "binomial")
summary(model)
```
Zauważamy, że wszystkie zmienne wpływają na wynik zmiennej objaśnianej.

Wykonujemy również macierz korelacji:
```{r}
data$diagnosis <- as.numeric(data$diagnosis)

correlation <- cor(data[, c("mean_radius", "mean_texture", "mean_perimeter","mean_area", "mean_smoothness", "diagnosis")])


corrplot(correlation, method = "color", type = "upper", order = "hclust")
data$diagnosis <- as.factor(data$diagnosis)
data$diagnosis <- factor(data$diagnosis, levels = c( 1, 2), labels = c( 1, 0))
```
Najbardziej optymalna byłaby sytuacja, gdyby zmienne objaśniające były silnie skorelowane ze zmienną objaśnianą ale słabo skorelowane między sobą. Na powyższym wykresie widzimy że zmienna diagnosis jest stosunkowo silnie ujemnie skorelowana ze zmiennymi mean_area, mean_radius oraz mean_perimiter, a słabiej, choć również skorelowana ze zmiennymi    mean_smoothness oraz mean_texture. Widzimy również że korelacja nie zachodzi między tymi zmiennymi. Niestety zmienne które najbardziej wpływają na zmienną objaśnianą są ze sobą skorelowane, ale z racji iż posiadamy niewielką ilość zmiennych objaśniających nie będziemy ich usuwać.

##### Podstawowe statystyki na zbiorze danych i ocena zbalansowania zbioru danych

```{r}
describe(select(data, -diagnosis))
prop.table(table(data$diagnosis))
```
**mean_radius (Średnica guza)**:

Średnia średnicy guza wynosi 14.13, a odchylenie standardowe wynosi 3.52. Sugeruje to umiarkowany stopień zmienności w średnicy guza. Mediana wynosi 13.37, a średnia przycięta (bez wpływu wartości skrajnych) to 13.82, co wskazuje na ewentualne obecność wartości odstających, jednakże nie powinno być ich bardzo dużo bo mediana i średnia są do siebie zbliżone, co sugeruje również że rozkład tej zmiennej może być zbliżony do rozkładu normalnego.

**mean_texture (Średnia tekstura guza):**

Średnia tekstury guza wynosi 19.29, a odchylenie standardowe to 4.30. Rozproszenie jest względnie wysokie. Mediana (18.84) i średnia przycięta (19.04) są zbliżone, co sugeruje równomierny rozkład wartości jak i również możliwość istnienia kilku wartości odstających.

**mean_perimeter (Średni obwód guza):**

Średni obwód guza wynosi 91.97, a odchylenie standardowe wynosi 24.30, co wskazuje na duże zróżnicowanie w obwodzie guza. Mediana (86.24) i średnia przycięta (89.74) są niższe niż średnia arytmetyczna, co może świadczyć o wpływie wartości skrajnych.

**mean_area (Średnia powierzchnia guza):**

Średnia powierzchnia guza wynosi 654.89, a odchylenie standardowe to 351.91, co sugeruje znaczną zmienność. Mediana (551.10) jest znacząco niższa niż średnia arytmetyczna, co może wynikać z obecności wartości odstających o dużych powierzchniach guza. Zmienna ta wydaje się być znacznie bardziej zróżnicowana niż pozostałe zmienne, co sugeruje, że obszar guzów może mieć znaczący wpływ na analizę.

**mean_smoothness (Średnia gładkość guza):**

Średnia gładkość guza wynosi 0.10, a odchylenie standardowe to 0.01. Wartości te sugerują niewielką zmienność w gładkości guza. Skośność jest bliska zeru, co wskazuje na symetryczny rozkład. Wartości "mean_smoothness" wydają się być dość zbliżone, co wskazuje na mniejszą zmienność w gładkości guza.

##### Ocena zbalansowania zbioru danych
**diagnosis (diagnoza)**
37% danych to zmiany nowotworowe zdiagnozowane jako łagodne, a 63% danych to zmiany złośliwe. WIdzimy, że zbiór danych jest średnio zbalansowany, ponieważ występuje więcej wartości 1 niż 0. 

#### Przygotowanie danych
W celu przygotowania danych sprawdzamy, czy występują braki danych.
```{r}
any(is.na(data))
```
W zbiorze danych nie posiadamy braków.

Następnie wykonujemy wykresy pudełkowe aby sprawdzić, czy zmienne zawierają wartości odstające.
```{r}
data_positive <- data[data$diagnosis==1,]
data_negative <- data[data$diagnosis==0,]
par(mfrow = c(2, 3))
for (nazwa_zmiennej in colnames(data_positive)) {
  boxplot(data_positive[[nazwa_zmiennej]], main = nazwa_zmiennej)
}
for (nazwa_zmiennej in colnames(data_negative)) {
  boxplot(data_negative[[nazwa_zmiennej]], main = nazwa_zmiennej)
}
```


```{r}
par(mfrow = c(1, 1))
```
##### Usuwanie wartości odstających
```{r}
usuń_odstające <- function(dane) {
  for (nazwa_zmiennej in colnames(dane)) {
    if (nazwa_zmiennej != "diagnosis") {  # Sprawdzenie, czy kolumna nie jest "diagnosis"
      Q1 <- quantile(dane[[nazwa_zmiennej]], 0.25)
      Q3 <- quantile(dane[[nazwa_zmiennej]], 0.75)
      IQR_value <- Q3 - Q1

      # Określenie granic wartości odstających
      dolna_granica <- Q1 - 1.5 * IQR_value
      gorna_granica <- Q3 + 1.5 * IQR_value

      # Usunięcie wartości odstających
      dane <- dane[!(dane[[nazwa_zmiennej]] < dolna_granica | dane[[nazwa_zmiennej]] > gorna_granica), ]
    }
  }
  return(dane)
}

data_positive_clear<- usuń_odstające(data_positive)
data_negative_clear<- usuń_odstające(data_negative)
data_clear <- rbind(data_positive_clear, data_negative_clear)

```
Po usunięciu wartości odstających modele osiągają lepszą dokładność i czułość.


#### Podział na zbiór uczący oraz testowy
Zbiór został podzielony w proporcji 80:20
```{r}
index <- sample(nrow(data), 420, replace = F)
train <- data[index,]
test <- data[-index,]
```

Następnie porównujemy podstawowe statystyki w obu grupach.
```{r}
describe(select(train, -diagnosis))
prop.table(table(train$diagnosis))
describe(select(test, -diagnosis))
prop.table(table(test$diagnosis))
```
Średnie wartości praktycznie wszystkich zmiennych w zbiorach train i test różnią się od siebie, lecz patrząc na odchylenia standardowych tych zmiennych nie różnią się od siebie bardzo ( np średnia wartość zmiennej mean_radius wynosi 13.95 w zbiorze uczącycym oraz 14.63 w zbiorze testowym, ale odchylenia standardowe w obu grupach wynoszą 3.79 więc różnicę rzędu 0.68 jesteśmy w stanie zaakceptować). Zauważamy również że średnie wartości zmiennych są wyższe w zbiorze train. Największa różnica średniej i mediany występuje u zmiennej mean_area, co wskazuje na to, że w zbiorze treningowym znalazło się więcej obserwacji o wyższej średniej powierzchni guza. Jeśli chodzi o podział obserwacji na pozytywne oraz negatywne, w zbiorze treningowym mamy większy odsetek obserwacji pozytywnych (66%) niż w zbiorze testowym (52%), jednak różnica ta nie jest wielka.

##### Standaryzujemy dane
```{r}
train_glm<-train
train$mean_radius<- scale(train$mean_radius)
train$mean_texture<- scale(train$mean_texture)
train$mean_perimeter<- scale(train$mean_perimeter)
train$mean_area<- scale(train$mean_area)
train$mean_smoothness<- scale(train$mean_smoothness)

test_glm<- test
test$mean_radius<- scale(test$mean_radius)
test$mean_texture<- scale(test$mean_texture)
test$mean_perimeter<- scale(test$mean_perimeter)
test$mean_area<- scale(test$mean_area)
test$mean_smoothness<- scale(test$mean_smoothness)
```

#### Wykonanie modeli uczenia maszynowego

##### Model SVM

Pierwszym modelem, któy zdecydowałyśmy się stworzyć jest model SVM o jądrze liniowym. Jedynym parametrem, jaki można w tym modelu optymalizować to wartość kosztu, dlatego wykonałyśmy wykres obrazujący wartości dokładności i czułości w zależności od przyjętego poziomu kosztu.
```{r message=FALSE, warning=FALSE}
library(ggplot2)

costs <- seq(0.01, 2, by = 0.1)
results <- data.frame(cost = numeric(), accuracy = numeric(), sensitivity = numeric())

for (cost in costs) {
  model <- svm(formula = diagnosis ~ ., data = train, kernel = "linear", cost = cost)
  pred_train <- predict(model, newdata = train)
  conf_matrix <- confusionMatrix(pred_train, train$diagnosis, positive = "1")
  accuracy <- conf_matrix$overall["Accuracy"]
  sensitivity <- conf_matrix$byClass["Sensitivity"]
  results <- rbind(results, data.frame(cost = cost, accuracy = accuracy, sensitivity = sensitivity))
}

ggplot(results, aes(x = cost)) +
  geom_line(aes(y = accuracy, color = "Accuracy"), size = 1.5) +
  geom_line(aes(y = sensitivity, color = "Sensitivity"), size = 1.5) +
  labs(title = "Wyniki dla różnych kosztów",
       x = "Koszt", y = "Wartość") +
  scale_color_manual(name = "Wartość", values = c("Accuracy" = "blue", "Sensitivity" = "red")) +
  theme_minimal()

```
Po analizie wykresu zdecydowaliśmy się przyjąć wartość parametru kosztu na poziomie 0.41 ponieważ oferuje on dobry stosunek dokładności do czułości.

Następnie tworzymy model SVM z przyjętą wartością parametru kosztu oraz porównujemny wyniki na zbiorze uczącym i testowym
```{r}
model <- svm(formula = diagnosis ~ ., data = train, kernel = "linear", cost = 0.41)
pred_train <- predict(model, newdata = train)
conf_matrix_train <- confusionMatrix(pred_train, train$diagnosis, positive = "1")
accuracy_train <- conf_matrix_train$overall["Accuracy"]
sensitivity_train <- conf_matrix_train$byClass["Sensitivity"]
specificity_train <- conf_matrix_train$byClass["Specificity"]

pred_test <- predict(model, newdata = test)
conf_matrix_test <- confusionMatrix(pred_test, test$diagnosis, positive = "1")
accuracy_test <- conf_matrix_test$overall["Accuracy"]
sensitivity_test <- conf_matrix_test$byClass["Sensitivity"]
specificity_test <- conf_matrix_test$byClass["Specificity"]

comparison_table_svm <- data.frame(
  Dataset = c("Train", "Test"),
  Accuracy = c(accuracy_train, accuracy_test),
  Sensitivity = c(sensitivity_train, sensitivity_test),
  Specificity = c(specificity_train, specificity_test)
)

print(comparison_table_svm)
```
Zauważamy, że nasz model osiągnął zadowalające wyniki, dokładność czułość i specyficzność posiadają podobne wartości dla zbioru uczącego i testowego. Co zaskakujące, wartość czułości dla zbioru testowego wyszła wyższa niż dla zbioru testowego.
Sprawdzimy też, jakie byłyby wyniki gdybyśmy próbowali jeszcze dokładniej zbalansować nasze dane.
```{r include=FALSE}
prop.table(table(train$diagnosis))
over_train <- ovun.sample(diagnosis ~ ., data = train, method="over",p=0.50 )$data
prop.table(table(over_train$diagnosis))
under_train <- ovun.sample(diagnosis ~ ., data = train, method="under",p=0.50 )$data
prop.table(table(under_train$diagnosis))


over_train_glm <- ovun.sample(diagnosis ~ ., data = train_glm, method="over",p=0.50 )$data
prop.table(table(over_train_glm$diagnosis))
under_train_glm <- ovun.sample(diagnosis ~ ., data = train_glm, method="under",p=0.50 )$data
prop.table(table(under_train_glm$diagnosis))
```
Dla porónania wykonamy oversampling i undersampling, aby sprawdzić czy model będzie wtedy lepszy.

Undersampling:
```{r warning=FALSE}
model <- svm(formula = diagnosis ~ ., data = under_train, kernel = "linear", cost = 0.41)
pred_train <- predict(model, newdata = under_train)
conf_matrix_train <- confusionMatrix(pred_train, under_train$diagnosis, positive = "1")
accuracy_train <- conf_matrix_train$overall["Accuracy"]
sensitivity_train <- conf_matrix_train$byClass["Sensitivity"]
specificity_train <- conf_matrix_train$byClass["Specificity"]

pred_test <- predict(model, newdata = test)
conf_matrix_test <- confusionMatrix(pred_test, test$diagnosis, positive = "1")
accuracy_test <- conf_matrix_test$overall["Accuracy"]
sensitivity_test <- conf_matrix_test$byClass["Sensitivity"]
specificity_test <- conf_matrix_test$byClass["Specificity"]

comparison_table_svm_u <- data.frame(
  Dataset = c("Train", "Test"),
  Accuracy = c(accuracy_train, accuracy_test),
  Sensitivity = c(sensitivity_train, sensitivity_test),
  Specificity = c(specificity_train, specificity_test)
)

print(comparison_table_svm_u)
```
Oversampling:
```{r warning=FALSE}
model <- svm(formula = diagnosis ~ ., data = over_train, kernel = "linear", cost = 0.41)
pred_train <- predict(model, newdata = over_train)
conf_matrix_train <- confusionMatrix(pred_train, over_train$diagnosis, positive = "1")
accuracy_train <- conf_matrix_train$overall["Accuracy"]
sensitivity_train <- conf_matrix_train$byClass["Sensitivity"]
specificity_train <- conf_matrix_train$byClass["Specificity"]

pred_test <- predict(model, newdata = test)
conf_matrix_test <- confusionMatrix(pred_test, test$diagnosis, positive = "1")
accuracy_test <- conf_matrix_test$overall["Accuracy"]
sensitivity_test <- conf_matrix_test$byClass["Sensitivity"]
specificity_test <- conf_matrix_test$byClass["Specificity"]

comparison_table_svm_o <- data.frame(
  Dataset = c("Train", "Test"),
  Accuracy = c(accuracy_train, accuracy_test),
  Sensitivity = c(sensitivity_train, sensitivity_test),
  Specificity = c(specificity_train, specificity_test)
)

print(comparison_table_svm_o)
```
Porównamy wyniki uzyskane z lasów losowych oraz lasów losowych z zastosowaniem oversamplingu i undersamplingu.
```{r}
print(comparison_table_svm)
print(comparison_table_svm_u)
print(comparison_table_svm_o)

```
Przy użyciu metody SVM najlepsze wartości dokładności i czułości otrzymujemy przy zastosowaniu metody oversamplingu, a jednocześnie przy tej metodzie wartości czułości i specyficzności najmniej różnią się od siebie na zbiorze uczącym i testowym. Zwykły model SVM może być aż za bardzo dopasowany do danych uczących, bo różnica między zbiorem treningowym i testowym jest znacząca.


##### Lasy losowe
```{r}
model <- randomForest(diagnosis ~ ., data = train%>%slice_sample(prop=0.4), ntree = 100)
pred_train <- predict(model, newdata = train)
conf_matrix_train <- confusionMatrix(pred_train, train$diagnosis, positive = "1")
accuracy_train <- conf_matrix_train$overall["Accuracy"]
sensitivity_train <- conf_matrix_train$byClass["Sensitivity"]
specificity_train <- conf_matrix_train$byClass["Specificity"]

pred_test <- predict(model, newdata = test)
conf_matrix_test <- confusionMatrix(pred_test, test$diagnosis, positive = "1")
accuracy_test <- conf_matrix_test$overall["Accuracy"]
sensitivity_test <- conf_matrix_test$byClass["Sensitivity"]
specificity_test <- conf_matrix_test$byClass["Specificity"]

comparison_table_rForest <- data.frame(
  Dataset = c("Train", "Test"),
  Accuracy = c(accuracy_train, accuracy_test),
  Sensitivity = c(sensitivity_train, sensitivity_test),
  Specificity = c(specificity_train, specificity_test)
)

print(comparison_table_rForest)
```
Dla porónania wykonamy oversampling, aby sprawdzić czy model będzie wtedy lepszy.

Undersampling:
```{r warning=FALSE}
model <- randomForest(diagnosis ~ ., data = under_train%>%slice_sample(prop=0.4), ntree = 100)
pred_train <- predict(model, newdata = under_train)
conf_matrix_train <- confusionMatrix(pred_train, under_train$diagnosis, positive = "1")
accuracy_train <- conf_matrix_train$overall["Accuracy"]
sensitivity_train <- conf_matrix_train$byClass["Sensitivity"]
specificity_train <- conf_matrix_train$byClass["Specificity"]

pred_test <- predict(model, newdata = test)
conf_matrix_test <- confusionMatrix(pred_test, test$diagnosis, positive = "1")
accuracy_test <- conf_matrix_test$overall["Accuracy"]
sensitivity_test <- conf_matrix_test$byClass["Sensitivity"]
specificity_test <- conf_matrix_test$byClass["Specificity"]

comparison_table_rForest_u <- data.frame(
  Dataset = c("Train", "Test"),
  Accuracy = c(accuracy_train, accuracy_test),
  Sensitivity = c(sensitivity_train, sensitivity_test),
  Specificity = c(specificity_train, specificity_test)
)

print(comparison_table_rForest_u)
```
Oversampling:
```{r warning=FALSE}
model <- randomForest(diagnosis ~ ., data = over_train%>%slice_sample(prop=0.4), ntree = 100)
pred_train <- predict(model, newdata = over_train)
conf_matrix_train <- confusionMatrix(pred_train, over_train$diagnosis, positive = "1")
accuracy_train <- conf_matrix_train$overall["Accuracy"]
sensitivity_train <- conf_matrix_train$byClass["Sensitivity"]
specificity_train <- conf_matrix_train$byClass["Specificity"]

pred_test <- predict(model, newdata = test)
conf_matrix_test <- confusionMatrix(pred_test, test$diagnosis, positive = "1")
accuracy_test <- conf_matrix_test$overall["Accuracy"]
sensitivity_test <- conf_matrix_test$byClass["Sensitivity"]
specificity_test <- conf_matrix_test$byClass["Specificity"]

comparison_table_rForest_o <- data.frame(
  Dataset = c("Train", "Test"),
  Accuracy = c(accuracy_train, accuracy_test),
  Sensitivity = c(sensitivity_train, sensitivity_test),
  Specificity = c(specificity_train, specificity_test)
)

print(comparison_table_rForest_o)
```
Porównamy wyniki uzyskane z lasów losowych oraz lasów losowych z zastosowaniem oversamplingu i undersamplingu.
```{r}
print(comparison_table_rForest)
print(comparison_table_rForest_u)
print(comparison_table_rForest_o)
```
Co ciekawe, zauważamy że wyniki na zbiorze train przyjmują różne wartości dla wszystkich metod, jednak na zbiorze testowym wartości dokładności czułości i specyficzności uzyskanych za pomocą zwykłych lasów losowych jak i lasów losowych z użyciem undersamplingu przyjmują te same wartości, mimo że na zbiorze testowym przy użyciu undersamplingu uzyskaliśmy wyższe wartości dokładności i specyficzności. Przy użyciu oversamplingu otrzymaliśmy zdecydowanie lepsze wartości na zbiorze testowym w poróznaniu ze zwykłymi lasami losowymi, ale na zbiorze testowym model stworzony na rozszerzonym zbiorze danych osiągnął gorszą specyficzność, jednakże lepszą (jak i również najlepszą) czułość, dlatego biorąc pod uwagę że mamy styczność z danymi medycznymi jako najlepszy wybierzemy model z najwyższą czułością, więc model stworzony w oparciu o oversampled data.

##### Model regersji logistycznej

Ostatnim modelem utworzonym w nasym projekcie będzie model regresji logistycznej
```{r warning=FALSE}
model<- caret::train(diagnosis ~ ., data = train, method = "glm")
summary(model)
pred_train <- predict(model, newdata = train)
conf_matrix_train <- confusionMatrix(pred_train, train$diagnosis, positive = "1")
accuracy_train <- conf_matrix_train$overall["Accuracy"]
sensitivity_train <- conf_matrix_train$byClass["Sensitivity"]
specificity_train <- conf_matrix_train$byClass["Specificity"]

pred_test <- predict(model, newdata = test)
conf_matrix_test <- confusionMatrix(pred_test, test$diagnosis, positive = "1")
accuracy_test <- conf_matrix_test$overall["Accuracy"]
sensitivity_test <- conf_matrix_test$byClass["Sensitivity"]
specificity_test <- conf_matrix_test$byClass["Specificity"]

comparison_table_glm <- data.frame(
  Dataset = c("Train", "Test"),
  Accuracy = c(accuracy_train, accuracy_test),
  Sensitivity = c(sensitivity_train, sensitivity_test),
  Specificity = c(specificity_train, specificity_test)
)

print(comparison_table_glm)
```
Porównamy również, jak model regresji logistycznej działa na danych na zbiorze testowym rozszerzonym przez undersampling i oversampling.

Oversampling
```{r}
```
Undersampling:
```{r warning=FALSE}
model<- caret::train(diagnosis ~ ., data = over_train, method = "glm")
pred_train <- predict(model, newdata = over_train)
conf_matrix_train <- confusionMatrix(pred_train, over_train$diagnosis, positive = "1")
accuracy_train <- conf_matrix_train$overall["Accuracy"]
sensitivity_train <- conf_matrix_train$byClass["Sensitivity"]
specificity_train <- conf_matrix_train$byClass["Specificity"]

pred_test <- predict(model, newdata = test)
conf_matrix_test <- confusionMatrix(pred_test, test$diagnosis, positive = "1")
accuracy_test <- conf_matrix_test$overall["Accuracy"]
sensitivity_test <- conf_matrix_test$byClass["Sensitivity"]
specificity_test <- conf_matrix_test$byClass["Specificity"]

comparison_table_glm_o <- data.frame(
  Dataset = c("Train", "Test"),
  Accuracy = c(accuracy_train, accuracy_test),
  Sensitivity = c(sensitivity_train, sensitivity_test),
  Specificity = c(specificity_train, specificity_test)
)

print(comparison_table_glm_o)
```



Undersampling

```{r warning=FALSE}
model<- caret::train(diagnosis ~ ., data = under_train, method = "glm")
pred_train <- predict(model, newdata = under_train)
conf_matrix_train <- confusionMatrix(pred_train, under_train$diagnosis, positive = "1")
accuracy_train <- conf_matrix_train$overall["Accuracy"]
sensitivity_train <- conf_matrix_train$byClass["Sensitivity"]
specificity_train <- conf_matrix_train$byClass["Specificity"]

pred_test <- predict(model, newdata = test)
conf_matrix_test <- confusionMatrix(pred_test, test$diagnosis, positive = "1")
accuracy_test <- conf_matrix_test$overall["Accuracy"]
sensitivity_test <- conf_matrix_test$byClass["Sensitivity"]
specificity_test <- conf_matrix_test$byClass["Specificity"]

comparison_table_glm_u <- data.frame(
  Dataset = c("Train", "Test"),
  Accuracy = c(accuracy_train, accuracy_test),
  Sensitivity = c(sensitivity_train, sensitivity_test),
  Specificity = c(specificity_train, specificity_test)
)

print(comparison_table_glm_u)
```
Porównamy wyniki uzyskane z modelu opartego na  regresji logistycznej oraz z zastosowaniem oversamplingu i undersamplingu.
```{r}
print(comparison_table_glm)
print(comparison_table_glm_u)
print(comparison_table_glm_o)
```
Tym razem na zbiorze treningowym każdy model daje inne wyniki, najlepszą dokładność uzyskał zwykły model regresji lo, najlepszą czułość model z użyciem metody undersampling a najwyższą specyficzność model znów zwykły model regresji logistycznej. Na zbiorze testowym najlepszą dokładność uzyskał model z użyciem metody undersampling, najlepszą (ale taką samą) czułość uzyskały modele z użyciem metody undersampling i oversampling a najwyższą specyficzność model znów zwykły model regresji logistycznej. Podsumowując za najlepszy uznajemy model uzyskany metodą undersampling, bo przyjmuje on najwyższą czułość, przy możliwie wysokim poziomie dokładności ( wyższym niż przy użyciu metody oversampling).